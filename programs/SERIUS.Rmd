---
title: "Gene Network"
author: "Jan Izquierdo i Ramos"
date: "2025-02-03"
always_allow_html: true
output: 
  tint::tintHtml:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, fig.width=16, fig.height=8)
knitr::opts_chunk$set(purl = TRUE)
```

```
Warning: the term species may be used across the code and comments when meaning lines (or genotypes)
```

# Loading

## Load libraries

```{r loadLib, results='hide', message=FALSE}
library(WGCNA)
allowWGCNAThreads()
library(randomcoloR)
library(edgeR)
library(tidyverse)
library(dplyr)
library(grid)
library(gridExtra)
#devtools::install_github("kevinblighe/CorLevelPlot")
library(CorLevelPlot)
library(ggpubr)
library(plotly)
library(factoextra)
library(umap)
library(heatmaply)
```

## Load the data

Import the csv and metadata files

```{r dtLoad}
dataPath<-paste0("../data/wlen/data_wlen.csv")
metadataPath<-paste0("../data/metadata.txt")
moduledata<-"./geneModule.txt"

dataNL<-read.delim(dataPath, row.names=1, stringsAsFactors=TRUE)
metadata<-read.delim(metadataPath, header=T, row.names=1, stringsAsFactors=TRUE)
genemodule<-read.table(moduledata, stringsAsFactors=TRUE)

colnames(metadata)<-c("specie", "quality", "tissue_abv", "rep", "location")

moduleList<-levels(genemodule$modules)
table(genemodule$modules)
#Data selection
ME<-"lightcyan" #green
genemodule$genes<-rownames(genemodule)
MEgeneList<-rownames(genemodule[genemodule$modules==ME,])
dataNL<-dataNL[MEgeneList,]
```

# Data preparation

## Get lengths vector

If the provided data contains the length of the genes, extract them to a vector

```{r getLen}
if ("Length" %in% colnames(dataNL)){
  length_vec<-dataNL$Length
}
```

## Fix the data

Search for dimensional disparities

```{r dataSearch}
dim(dataNL)
dim(metadata)
```

### Match metadata rows to data columns

Match which samples appear in the data and metadata

```{r dataMatch}
dataMatcher<-function(data, metadata){
  options(warn=-1)
  
  cat("Data begins with:" , dim(data))
  cat("\nMetadata begins with:", dim(metadata))
  #Match data to metadata
  data <- data[, order(colnames(data))]
  metadata <- metadata[order(rownames(metadata)), ]
  
  cat("\nColumns data = Rows of metatdata?", all(rownames(metadata) == colnames(data)))
  #If TRUE, columns of data and rows of metadata are matched
  
  cat("\nRemove the excess from data")
  data<-data[,colnames(data) %in% rownames(metadata)] #remove data not present in metadata
  cat("\nData end with:" , dim(data))
  
  cat("\nRemove the excess from metadata")
  metadata<-metadata[rownames(metadata) %in% colnames(data),] #remove metadata not present in data
  cat("\nMetadata end with:" , dim(metadata), "\n")
  
  options(warn=0)
  return(list(data, metadata))
}

jointData<-dataMatcher(dataNL, metadata)

dataNL<-jointData[[1]]
metadata<-jointData[[2]]
```

### Outlier check

Check for outlier genes

```{r OutlierGenes}
outDetect<-goodSamplesGenes(t(dataNL))

table(outDetect$goodGenes) #False genes are outliers
table(outDetect$goodSamples) #All samples are True = no outliers
```

```{r OutlierGeRemove}
dataNL<-dataNL[outDetect$goodGenes==TRUE,] #remove ouliers

if (exists("length_vec")){ #only if it exists
  length_vec<-length_vec[outDetect$goodGenes==TRUE] #if length_vec exists remove outliers from there as well
}

```

## Replicate joining

```{r replSpecieName}
#add the name of the species to the replicate, to be able to differentiate it

#save location
metadata$org_location<-as.factor(metadata$location)
#create specialized location
metadata$location<-paste(metadata$specie, metadata$location, sep="_")
metadata$location<-as.factor(metadata$location)

```

```{r replJoin}
rsumer<-function(data, metadata, tissue_name){ #calculates the mean of all columns that belong to a location (ex: mean of all COB columns)
  
  loc_mdata<-metadata[metadata$location == tissue_name, ] #filter metadata tissue (get metadata of location only)
  
  data<-data[,colnames(data) %in% rownames(loc_mdata)] #get data of location only, based on metadata
  
  if (1<ncol(data.frame(data))){ #If there's only 1 replicate, don't try to do the mean (it gives error)
    data<-rowMeans(data) #calculate mean for each gene out of the locations(replicates)
  }
  
  return(as.data.frame(data))
}

tissue_data<-levels(metadata$location) #get list of tissue names

d_joint<-sapply(tissue_data, function(tissue_name) rsumer(dataNL, metadata, tissue_name)) #returns an array where each entry is a column with the mean data of the replicates (rows are genes)

repl_data<-as.data.frame(d_joint) #data joint by replicate

colnames(repl_data) = gsub(pattern = "*.data", replacement = "", x = tolower(colnames(repl_data))) #get column names to be only location

rownames(repl_data)<-rownames(dataNL) #rename rows to be genes again

```

Create replicate metadata
```{r replMeta}
#Create a column for the location+species
repl_meta<-as.data.frame(colnames(repl_data))
colnames(repl_meta)<-c("location")

#Split the created column and add 2 columns to repl_meta, one for the species and one for the tissue
temp_meta<-data.frame(t(data.frame(strsplit(as.character(repl_meta$location), "_"))))
repl_meta<-data.frame(repl_meta$location, temp_meta$X1, temp_meta$X2)
colnames(repl_meta)<-c("location", "species", "org_location")
```

### RPKM

Filter out empty columns (sum==0)
```{r RPKMfilter}
keep<-colSums(repl_data)!=0
repl_data<-repl_data[,keep]
```


Only if we have the gene lengths
```{r RPKMnorm}
if (exists("length_vec")){ #if we have lengths
 length_vec<-data.frame(Length=length_vec) #convert to dataframe

  dge <- DGEList(repl_data, genes=length_vec) #use edgeR for normalization

  dge <- calcNormFactors(dge)
  Nrepl_data <- rpkm(dge, log=TRUE)

  Nrepl_data<-as.data.frame(Nrepl_data)
  
  NormType<-"RPKM"
}
```

#### Low expression removal

```{r RPKMlExpRm}
#Filter low expression genes
keep<-apply(Nrepl_data, 1, max)>=0 #keep genes where the counts for at least one replicate are of at least 1 (0 because of log)
Nrepl_data<-Nrepl_data[keep,]

#Nrepl to data frame
Nrepl_data<-as.data.frame(Nrepl_data) #evetually transpose

#More genes kept after normalization and filtering than cpm
```

# Gene expression frequency

```{r gExpr}
set.seed(42)

#Choose which genes to use
geneNumber<-round(nrow(Nrepl_data)/100 *10) #1% of genes
#get random genes
chosenGenes<-sample(rownames(Nrepl_data), geneNumber)
#obtain genes data
dataSection<-Nrepl_data[chosenGenes,]

#Save tissues
tissues<-colnames(dataSection)
#create genes column
dataSection$Gene<-rownames(dataSection)

#reshape data
long_data<-reshape(dataSection, direction="long",
                   v.names="Expression",
                   timevar="Tissue",
                   varying=tissues,
                   times=names(dataSection)[1:(ncol(dataSection)-1)]) #names(dataSection)[1:(ncol(dataSection)-1)]

ggplot(long_data, aes(x=Tissue, y=Expression, group=Gene)) +
  geom_line(size=1, alpha=0.2) +
  labs(x="Tissues", y="Expression", title=paste0("Module: ", ME, " at genes: ", geneNumber))+
  theme_classic()+
  theme(axis.text.x=element_text(angle=45, hjust=1, size=10),
        axis.text.y=element_text(size=12),
        legend.position="none",
        axis.ticks=element_line(colour="black"),
        axis.ticks.length=unit(8, "pt"),
        panel.grid.major=element_line(color="red",
                                        size=0.5))

ggplot(long_data, aes(x=Tissue, y=Expression, group=Gene)) +
  geom_line(size=1, alpha=0.1) +
  labs(x="Tissues", y="Expression", title=paste0("Module: ", ME, " at genes: ", geneNumber))+
  theme_classic()+
  theme(axis.text.x=element_text(angle=45, hjust=1, size=10),
        axis.text.y=element_text(size=12),
        legend.position="none",
        axis.ticks=element_line(colour="black"),
        axis.ticks.length=unit(8, "pt"))

```

# Correlation

```{r corPlot}
Nrepl_dataT<-as.data.frame(t(Nrepl_data))
#correaltion table
geneCor<-cor(Nrepl_dataT, method="pearson")

#Threshold choosing
ths<-seq(0,1, by=0.05)

ths_effect<- data.frame(ths, count=sapply(ths, function(t) sum(abs(geneCor)>t))) #count amount of correlations (all positive, as they all contribute)

ggplot(ths_effect, aes(x=ths, y=count))+
  geom_line(color=ME, size=2)+
  labs(x="Threshold", y="Correlaton count")+
  geom_point(size=4)+
  theme_bw()

threshold<-0.7

```

## Correlation distribution

```{r}
#select above the diagonal
cor_vals<-geneCor[upper.tri(geneCor)]

#convert to data frame
cor_df<-data.frame(Correlation=cor_vals)

#PLot distribution
ggplot(cor_df, aes(x=Correlation))+
  geom_histogram(binwidth=0.05, fill=ME, color="black")+
  labs(title="Distribution of correlation values",
       x="Correlation Coefficient",
       y="Frequency")+
  theme_minimal()
```
KMEANS WITH EXPRESSION
zsore
pca
variance analysis with different pcas
clustering
@@ Otherwise:
save corTable and load it to networkX

# Preclustering
```{r pca-prep}
set.seed(42)
#Z-score scaling
sc_data<-scale(Nrepl_data)
#pca
sc_pca<-prcomp(sc_data, scale=TRUE)

#pca plot
ggplot(sc_pca$x, aes(x=PC1, y=PC2))+
  geom_point(size=3)+
  scale_shape_manual(values = c(19, 1, 2, 15, 8))+
  #scale_size_manual(values = c(2, 6))+
  #scale_color_brewer(palette="Dark2")+
  labs(title="Normalized PCA")+
  theme_minimal()
```

## PC Component cutoff
```{r pca_components}
#pca PC effect
pca_sum<-summary(sc_pca)
pca_data<-as.data.frame(t(pca_sum$importance))

c_var<-pca_data$`Cumulative Proportion`
PC_c<-1:length(c_var)
```

Calculate pc component cutoff
```{r}
#buscar cutoff
plot(pca_data$`Cumulative Proportion`, xlab="PC component number", ylab="Cumulative variance")
abline(v=PC_c[1], col="red")
abline(h=c_var[length(c_var)], col="red")
```


```{r}
#function to calculate distance vector (distance to corner)
##reprogram to do distance to any point?
dVector_calculator<-function(xn, yn){
  #normalize values
  #x
  x_min<-min(xn)
  x_range<-max(xn)-x_min
  xn_scaled<-(xn-x_min)/x_range
  #y
  y_min<-min(yn)
  y_range<-max(yn)-y_min
  yn_scaled<-(yn-y_min)/y_range
  
  #define corner of the triangle (normalized)
  destination_y<-destination_y<-yn_scaled[length(yn_scaled)]
  destination_x<-xn_scaled[1]
  
  #calculate distances
  d_x<-(xn_scaled-destination_x)**2
  d_y<-(yn_scaled-destination_y)**2
  d_vector<-sqrt(d_x+d_y)
}

distance_vector<-dVector_calculator(PC_c, c_var)
plot(distance_vector)

#Objective is closest point to corner (closest to y=0 in plot)
#So the objective is min distance
which.min(distance_vector)

#####
plot(pca_data$`Cumulative Proportion`, xlab="PC component number", ylab="Cumulative variance")
abline(v=18, col="red")


e<-data.frame(x=xn_scaled, y=yn_scaled)
ggplot(e, aes(x=x, y=y))+geom_point()
```


```{r}
d<-as.data.frame(scale(emptyL))
plot(d$X0)
lines(d$X0.21705)

```


## Clustering

```{r cutoff}
#keep components up to the cutoff
ctf_pca<-as.data.frame(sc_pca$x[,1:pc_cutoff])
##aquests son valors transformats?? scikit =fit_transform es el mateix en prcomp q $x? o he d efer .predict?
```


### Kmeans clustering
```{r}
d_pca<-prcomp(ctf_pca)
kres<-kmeans(d_pca$x, 6)

plot(d_pca$x, col=kres$cluster, lwd=3)
```

Elbow for kmeans
```{r}

```

### dbscan
```{r}
#minPts=4 (d+1 or d*2)
mp<-ncol(sc_pca$x)+1
#eps: elbow plot, choose shaprest point in bend
kNNdistplot(sc_pca$x, k=mp)
e<-350

dbres<-dbscan(d_pca$x, eps=e, minPts=mp)
plot(d_pca$x, col=dbres$cluster+1, lwd=3)
```



# Clustering
```{r}
kmeans_res<-kmeans(Nrepl_data, 7)
#plot(scale(Nrepl_data), col=kmeans_res$cluster) #only scaled because if not plot margins too large
Ndt_umap<-umap(Nrepl_data)
plot(Ndt_umap$layout, col=kmeans_res$cluster)
```

Choosing the right clustering method:
K-means and dbscan are less efficient when in high dimensionality


```{r}
library(dbscan)

total_means<-c()

for (k in 1:15){
  dbscan_res<-dbscan(Nrepl_data, eps=k, minPts=3) #eps=neighborhood size
  mean_res<-length(unique(dbscan_res$cluster))
  total_means<-c(total_means, mean_res)
}
plot(1:15, total_means, xlab="Distance between clusters", type="b")

dbscan_res<-dbscan(Nrepl_data, eps=3, minPts=5)

plot(scale(Nrepl_data), col=dbscan_res$cluster)

plot(dt_umap$layout, col=dbscan_res$cluster)
```

# Saving the network
