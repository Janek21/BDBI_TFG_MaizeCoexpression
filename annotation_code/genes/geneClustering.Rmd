---
title: "Gene Network"
author: "Jan Izquierdo i Ramos"
date: "2025-02-03"
always_allow_html: true
output: 
  tint::tintHtml:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, fig.width=16, fig.height=8)
knitr::opts_chunk$set(root.dir = "..") #does not work
knitr::opts_chunk$set

getwd()
```

```
Warning: the term species may be used across the code and comments when meaning lines (or genotypes)
```

# Loading

## Load libraries

```{r loadLib, results='hide', message=FALSE}
library(WGCNA)
allowWGCNAThreads()
library(randomcoloR)
library(edgeR)
library(tidyverse)
library(dplyr)
library(grid)
library(gridExtra)
#devtools::install_github("kevinblighe/CorLevelPlot")
library(CorLevelPlot)
library(ggpubr)
library(plotly)
library(factoextra)
library(umap)
library(heatmaply)
library(dbscan)
library(reticulate)
reticulate::use_python("/usr/bin/python3", required=TRUE)
```

## Load the data

Import the csv and metadata files

```{r dtLoad}
setwd("..") #set path to general annotation_code directory

dataPath<-paste0("../data/wlen/data_wlen.csv")
metadataPath<-paste0("../data/metadata.txt")
moduledata<-"../coexpression_code/geneModule.txt"

dataNL<-read.delim(dataPath, row.names=1, stringsAsFactors=TRUE)
metadata<-read.delim(metadataPath, header=T, row.names=1, stringsAsFactors=TRUE)
genemodule<-read.table(moduledata, stringsAsFactors=TRUE)

colnames(metadata)<-c("specie", "quality", "tissue_abv", "rep", "location")

moduleList<-levels(genemodule$modules)
table(genemodule$modules)
#Data selection
ME<-"darkred" #green #lightcyan #turquoise
genemodule$genes<-rownames(genemodule)
MEgeneList<-rownames(genemodule[genemodule$modules==ME,])
dataNL<-dataNL[MEgeneList,]
```

# Data preparation

## Get lengths vector

If the provided data contains the length of the genes, extract them to a vector

```{r getLen}
if ("Length" %in% colnames(dataNL)){
  length_vec<-dataNL$Length
}
```

## Fix the data

Search for dimensional disparities

```{r dataSearch}
dim(dataNL)
dim(metadata)
```

### Match metadata rows to data columns

Match which samples appear in the data and metadata

```{r dataMatch}
dataMatcher<-function(data, metadata){
  options(warn=-1)
  
  cat("Data begins with:" , dim(data))
  cat("\nMetadata begins with:", dim(metadata))
  #Match data to metadata
  data <- data[, order(colnames(data))]
  metadata <- metadata[order(rownames(metadata)), ]
  
  cat("\nColumns data = Rows of metatdata?", all(rownames(metadata) == colnames(data)))
  #If TRUE, columns of data and rows of metadata are matched
  
  cat("\nRemove the excess from data")
  data<-data[,colnames(data) %in% rownames(metadata)] #remove data not present in metadata
  cat("\nData end with:" , dim(data))
  
  cat("\nRemove the excess from metadata")
  metadata<-metadata[rownames(metadata) %in% colnames(data),] #remove metadata not present in data
  cat("\nMetadata end with:" , dim(metadata), "\n")
  
  options(warn=0)
  return(list(data, metadata))
}

jointData<-dataMatcher(dataNL, metadata)

dataNL<-jointData[[1]]
metadata<-jointData[[2]]
```

### Outlier check

Check for outlier genes

```{r OutlierGenes}
outDetect<-goodSamplesGenes(t(dataNL))

table(outDetect$goodGenes) #False genes are outliers
table(outDetect$goodSamples) #All samples are True = no outliers
```

```{r OutlierGeRemove}
dataNL<-dataNL[outDetect$goodGenes==TRUE,] #remove ouliers

if (exists("length_vec")){ #only if it exists
  length_vec<-length_vec[outDetect$goodGenes==TRUE] #if length_vec exists remove outliers from there as well
}

```

## Replicate joining

```{r replSpecieName}
#add the name of the species to the replicate, to be able to differentiate it

#save location
metadata$org_location<-as.factor(metadata$location)
#create specialized location
metadata$location<-paste(metadata$specie, metadata$location, sep="_")
metadata$location<-as.factor(metadata$location)

```

```{r replJoin}
rsumer<-function(data, metadata, tissue_name){ #calculates the mean of all columns that belong to a location (ex: mean of all COB columns)
  
  loc_mdata<-metadata[metadata$location == tissue_name, ] #filter metadata tissue (get metadata of location only)
  
  data<-data[,colnames(data) %in% rownames(loc_mdata)] #get data of location only, based on metadata
  
  if (1<ncol(data.frame(data))){ #If there's only 1 replicate, don't try to do the mean (it gives error)
    data<-rowMeans(data) #calculate mean for each gene out of the locations(replicates)
  }
  
  return(as.data.frame(data))
}

tissue_data<-levels(metadata$location) #get list of tissue names

d_joint<-sapply(tissue_data, function(tissue_name) rsumer(dataNL, metadata, tissue_name)) #returns an array where each entry is a column with the mean data of the replicates (rows are genes)

repl_data<-as.data.frame(d_joint) #data joint by replicate

colnames(repl_data) = gsub(pattern = "*.data", replacement = "", x = tolower(colnames(repl_data))) #get column names to be only location

rownames(repl_data)<-rownames(dataNL) #rename rows to be genes again

```

Create replicate metadata
```{r replMeta}
#Create a column for the location+species
repl_meta<-as.data.frame(colnames(repl_data))
colnames(repl_meta)<-c("location")

#Split the created column and add 2 columns to repl_meta, one for the species and one for the tissue
temp_meta<-data.frame(t(data.frame(strsplit(as.character(repl_meta$location), "_"))))
repl_meta<-data.frame(repl_meta$location, temp_meta$X1, temp_meta$X2)
colnames(repl_meta)<-c("location", "species", "org_location")
```

### RPKM

Filter out empty columns (sum==0)
```{r RPKMfilter}
keep<-colSums(repl_data)!=0
repl_data<-repl_data[,keep]
```


Only if we have the gene lengths
```{r RPKMnorm}
if (exists("length_vec")){ #if we have lengths
 length_vec<-data.frame(Length=length_vec) #convert to dataframe

  dge <- DGEList(repl_data, genes=length_vec) #use edgeR for normalization

  dge <- calcNormFactors(dge)
  Nrepl_data <- rpkm(dge, log=TRUE)

  Nrepl_data<-as.data.frame(Nrepl_data)
  
  NormType<-"RPKM"
}
```

#### Low expression removal

```{r RPKMlExpRm}
#Filter low expression genes
keep<-apply(Nrepl_data, 1, max)>=0 #keep genes where the counts for at least one replicate are of at least 1 (0 because of log)
Nrepl_data<-Nrepl_data[keep,]

#Nrepl to data frame
Nrepl_data<-as.data.frame(Nrepl_data) #evetually transpose

#More genes kept after normalization and filtering than cpm
```

# Gene expression frequency

```{r gExpr}
set.seed(42)

#Choose which genes to use
geneNumber<-round(nrow(Nrepl_data)/100 *10) #1% of genes
#get random genes
chosenGenes<-sample(rownames(Nrepl_data), geneNumber)
#obtain genes data
dataSection<-Nrepl_data[chosenGenes,]

#Save tissues
tissues<-colnames(dataSection)
#create genes column
dataSection$Gene<-rownames(dataSection)

#reshape data
long_data<-reshape(dataSection, direction="long",
                   v.names="Expression",
                   timevar="Tissue",
                   varying=tissues,
                   times=names(dataSection)[1:(ncol(dataSection)-1)]) #names(dataSection)[1:(ncol(dataSection)-1)]

ggplot(long_data, aes(x=Tissue, y=Expression, group=Gene)) +
  geom_line(size=1, alpha=0.2) +
  labs(x="Tissues", y="Expression", title=paste0("Module: ", ME, " at genes: ", geneNumber))+
  theme_classic()+
  theme(axis.text.x=element_text(angle=45, hjust=1, size=10),
        axis.text.y=element_text(size=12),
        legend.position="none",
        axis.ticks=element_line(colour="black"),
        axis.ticks.length=unit(8, "pt"),
        panel.grid.major=element_line(color="red",
                                        size=0.5))

ggplot(long_data, aes(x=Tissue, y=Expression, group=Gene)) +
  geom_line(size=1, alpha=0.1) +
  labs(x="Tissues", y="Expression", title=paste0("Module: ", ME, " at genes: ", geneNumber))+
  theme_classic()+
  theme(axis.text.x=element_text(angle=45, hjust=1, size=10),
        axis.text.y=element_text(size=12),
        legend.position="none",
        axis.ticks=element_line(colour="black"),
        axis.ticks.length=unit(8, "pt"))

```

# Correlation

```{r corThs}
Nrepl_dataT<-as.data.frame(t(Nrepl_data))
#correaltion table
geneCor<-cor(Nrepl_dataT, method="pearson")

geneList<-rownames(geneCor)

#Threshold choosing
ths<-seq(0,1, by=0.05)

ths_effect<- data.frame(ths, count=sapply(ths, function(t) sum(abs(geneCor)>t))) #count amount of correlations (all positive, as they all contribute)

ggplot(ths_effect, aes(x=ths, y=count))+
  geom_line(color=ME, size=2)+
  labs(x="Threshold", y="Correlaton count")+
  geom_point(size=4)+
  theme_bw()

#A good correlation threshold would be 0.7

```

## Correlation distribution

```{r corDistr}
#select above the diagonal
cor_vals<-geneCor[upper.tri(geneCor)]

#convert to data frame
cor_df<-data.frame(Correlation=cor_vals)

#PLot distribution
ggplot(cor_df, aes(x=Correlation))+
  geom_histogram(binwidth=0.05, fill=ME, color="black")+
  labs(title="Distribution of correlation values",
       x="Correlation Coefficient",
       y="Frequency")+
  theme_minimal()

#Tends to appear bimodal
```

# Preclustering
K-means and dbscan are less efficient when in high dimensionality, so we will reduce it through PCA and Principal Component selection (cutoff)

```{r pca-prep}
set.seed(42)
#Z-score scaling
sc_data<-scale(Nrepl_data)
#scaled pca
sc_pca<-prcomp(sc_data, scale=TRUE)

#pca plot
ggplot(sc_pca$x, aes(x=PC1, y=PC2))+
  geom_point(size=3)+
  theme_minimal()
```

## Principal Component cutoff
```{r pca_components}
#pca PC effect
pca_sum<-summary(sc_pca)
pca_data<-as.data.frame(t(pca_sum$importance))

c_var<-pca_data$`Cumulative Proportion`
PC_c<-1:length(c_var)
```

Calculate PC cutoff
```{r pc_cornerPlot}
#search cutoff
plot(pca_data$`Cumulative Proportion`, xlab="PC component number", ylab="Cumulative variance", main="Elbow plot for PC component cutoff", lwd=2)
abline(v=PC_c[1], col="red", lwd=2)
abline(h=c_var[length(c_var)], col="red", lwd=2)
```

Use the intersection point (corner of a triangle, so point which would be plotted a perfect elbow plot) as a point of reference and calulate all distances to it. Take the minium distance, the closes point to the reference.
```{r pc_cutoff}
#function to calculate distance vector (distance to "triangle" corner)
dVector_calculator<-function(xn, yn, point_x, point_y){
  #normalize values, including the corner
  #x
  x_min<-min(xn)
  x_range<-max(xn)-x_min
  xn_scaled<-(xn-x_min)/x_range
  destination_x<-(point_x-x_min)/x_range #target point x coords, scaled
  #y
  y_min<-min(yn)
  y_range<-max(yn)-y_min
  yn_scaled<-(yn-y_min)/y_range
  destination_y<-(point_y-y_min)/y_range #target point y coords, scaled
  
  #calculate distances
  d_x<-(xn_scaled-destination_x)**2
  d_y<-(yn_scaled-destination_y)**2
  d_vector<-sqrt(d_x+d_y)
  return(d_vector)
}

#define corner of the triangle
corner_x<-PC_c[1]
corner_y<-c_var[length(c_var)]

distance_vector<-dVector_calculator(PC_c, c_var, corner_x, corner_y)
plot(distance_vector, xlab="Position", ylab="Distance", main="Distances to corner")

#Objective is closest point to corner (closest to y=0 in plot)
#So the objective is min distance
pc_cutoff<-which.min(distance_vector)

plot(pca_data$`Cumulative Proportion`, xlab="PC component number", ylab="Cumulative variance", main="Elbow plot for PC component cutoff", lwd=2)
abline(v=pc_cutoff, col="blue", lwd=2)
```

Having obtained the cutoff point, take all PC before it into a new data frame
```{r cutoff_application}
ctf_data<-as.data.frame(sc_pca$x[,1:pc_cutoff])
##aquests son valors transformats?? scikit =fit_transform es el mateix en prcomp q $x? o he d efer .predict?
```


# Clustering

## Kmeans clustering
Define plot landscape
```{r km_pcaUmap}
set.seed(42)

shortened_pca<-prcomp(ctf_data)
shortened_umap<-umap(ctf_data, n_epochs=1500, min_dist=0.9, n_neighbors=35)
```

### Kmeans number of clusters selection
Elbow for kmeans
```{r km_elbowComputing}
set.seed(42)

total_means<-c()
for (k in 1:20){
  kmeans_res<-kmeans(ctf_data, k)
  mean_res<-mean(kmeans_res$withinss)
  total_means<-c(total_means, mean_res)
}

#png("../report/presentation/images/Elbow_kmeans2.png", width=1600, height=800)
plot(1:20, total_means, xlab="NÂº of clusters", ylab="Total means", main="Cluster elbow for Kmeans", type="b", lwd=2)
abline(v=1, col="red", lwd=2)
abline(h=total_means[length(total_means)], col="red", lwd=2)
#abline(v=km_num, col="blue", lwd=2)

#dev.off()
```

Follow the same methods for computing distances and choosing the best point as before.
```{r km_distanceVector}
#define corner coordinates
corner_x<-1
corner_y<-total_means[length(total_means)]

#Get distance from each point to corner
distance_vector<-dVector_calculator(1:20, total_means, corner_x, corner_y)

plot(distance_vector, xlab="Position", ylab="Distance", main="Distances to corner", lwd=2)

#choose number of clusters
km_num<-which.min(distance_vector)
```

Plot the obtained data
```{r kmeans_computation}
set.seed(42)

kres<-kmeans(ctf_data, km_num)

plot(shortened_umap$layout, col=kres$cluster, lwd=3)

#("../report/presentation/images/Kmean_pcaColor.png", width=1600, height=800)
plot(shortened_pca$x, col=kres$cluster, lwd=3, main=paste0("PCA colored according to kmeans centroids=", km_num))
#dev.off()
```


## DBscan
```{r dbscan_cluster}
mp<-ncol(ctf_data)+1
#eps: elbow plot, choose shaprest point in bend
kNNdistplot(ctf_data, k=mp)
#3 is chosen, as the other inflection points produce no clusters

dbres<-dbscan(ctf_data, eps=3, minPts=mp)
plot(shortened_umap$layout, col=dbres$cluster+1, lwd=3)
```
Clusters by KMeans and DBscan present very rough grain, too little detail, so community clustering is tested.

# Saving the network

```{r saveNW}
setwd("..") #set path to general annotation_code directory
write.table(geneCor, paste0("./correlation_tables/", ME, "_geneCorrelation.txt"), sep="\t")
```

## Louvain community detection (python)
Execute the python code together with the R code for correlation data parity (and process efficiency) and for better correlation threshold visibility
```{python def_functions}
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt

#function to set the correaltion table as a list of edges, which can be turned into a network
def corrCleaner(table, ths=0.7):
    #Join the table as gene to gene relations: rows are [gene1 gene2 correlationCoeff]
    corList=table.stack().reset_index()
    corList.columns=["G1", "G2", "coefficient"]
    
    #Take out the self-loops
    corList_noSL=corList[corList["G1"]!=corList["G2"]]
    #filter by threshold (observed in genCor_clustering script(plot))
    correlations=corList_noSL[corList_noSL["coefficient"].abs()>=ths]
    
    return correlations

#transform an edgelist into a network
def nwLoad(edgelist):
    nw=nx.from_pandas_edgelist(edgelist, "G1", "G2", edge_attr="coefficient")
    return nw

#draw and show/save a network
def nwShow(nw, gr_title="No module", filename=None):
    plt.figure()  
    #plt.title(gr_title)
    nx.draw(nw,node_size=50)
    #Show or save
    if filename!=None:
        plt.savefig(filename)
    else:
        plt.show()
    plt.close()
    
```

```{python def_vars}
module=r.ME #get current module
arrayCor=r.geneCor #gene correlation table, read as array
gNames=list(r.geneList) #list of gene names
#transform the array into a table, give rownames and colnames as genes
tbCor=pd.DataFrame(arrayCor, columns=gNames, index=gNames)
```

Louvain community detection
```{python comm_clustering}
#get an edgelist(filterd by threshold) from the correaltion table
cor_edgelist=corrCleaner(tbCor, 0.6) #0.6
#create and draw the network
nw=nwLoad(cor_edgelist)
nwShow(nw, gr_title=module)

#Find communities in the network
louvain_comms = nx.community.louvain_communities(nw, weight="correlation", seed=42)
print(f"Number of communities: {len(louvain_comms)}")

#prepare for subplots
fig, axes=plt.subplots()

#Draw each community
for i, comm in enumerate(louvain_comms):
  if len(comm)>4: #ensure no single nodes
    subgraph=nw.subgraph(comm)
    ax=axes[i]
    ax.set_title(f"Community {i}")
    nx.draw(subgraph, with_labels=False, node_color=f"C{i}", node_size=100, font_size=6, edge_color="gray", linewidths=1, edgecolors='black', ax=ax)

plt.tight_layout()
plt.show()
#plt.savefig(f"./graphFigs/{i}_{module}.png")
        
#red 0.6
#turquoise 0.6
```

Community clustering presents finer grain clusters, which are more biologically relevant than the ones calculated by KMeans and DBscan